{
  "name": "blob_to_delta_transformer",
  "properties": {
    "nbformat": 4,
    "nbformat_minor": 2,
    "bigDataPool": {
      "referenceName": "fredsparkpool",
      "type": "BigDataPoolReference"
    },
    "sessionProperties": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2",
        "spark.autotune.trackingId": "blob_to_delta_transformer"
      }
    },
    "metadata": {
      "saveOutput": true,
      "enableDebugMode": false,
      "kernelspec": {
        "name": "synapse_pyspark",
        "display_name": "Synapse PySpark"
      },
      "language_info": {
        "name": "python"
      },
      "a365ComputeOptions": {
        "id": "/subscriptions/6f928fec-8d15-47d7-b27b-be8b568e9789/resourceGroups/MTWS_Synapse/providers/Microsoft.Synapse/workspaces/externaldata/bigDataPools/fredsparkpool",
        "name": "fredsparkpool",
        "type": "Spark",
        "endpoint": "https://externaldata.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/fredsparkpool",
        "auth": {
          "type": "AAD",
          "authResource": "https://dev.azuresynapse.net"
        },
        "sparkVersion": "3.3",
        "nodeCount": 4,
        "cores": 8,
        "memory": 56,
        "automaticScaleJobs": false
      },
      "sessionKeepAliveTimeout": 30
    },
    "cells": [
      {
        "cell_type": "code",
        "metadata": {
          "collapsed": false
        },
        "source": [
          "\"\"\"\n",
          "Azure Synapse Notebook - Blob to Delta Transformation\n",
          "Converts raw FRED parquet files to optimized Delta Lake table + Direct Analysis\n",
          "\n",
          "Run in: Azure Synapse Spark Pool\n",
          "Resources: Medium (4-8 cores recommended)\n",
          "\n",
          "Features:\n",
          "- Time-based partitioning (year, month) for optimal query performance\n",
          "- Z-ordering on series_id and release_id\n",
          "- Comprehensive quality control checks\n",
          "- Performance benchmarking\n",
          "- Direct series analysis (trends, statistics, coverage)\n",
          "\"\"\"\n",
          "\n",
          "from pyspark.sql import SparkSession\n",
          "from pyspark.sql.functions import (\n",
          "    col, lit, substring, year, month, count, countDistinct,\n",
          "    min as spark_min, max as spark_max, avg, stddev,\n",
          "    when, datediff, to_date, lag, first, last\n",
          ")\n",
          "from pyspark.sql.window import Window\n",
          "from delta.tables import DeltaTable\n",
          "import time\n",
          "\n",
          "# Configuration - PARAMETERIZED (can be overridden by orchestrator)\n",
          "STORAGE_ACCOUNT = dbutils.widgets.get(\"storage_account\") if \"dbutils\" in dir() else \"gzcstorageaccount\"\n",
          "CONTAINER = dbutils.widgets.get(\"container\") if \"dbutils\" in dir() else \"macroeconomic-maintained-series\"\n",
          "VINTAGE_DATE = dbutils.widgets.get(\"vintage_date\") if \"dbutils\" in dir() else \"2025-11-21\"\n",
          "RELEASE_ID = dbutils.widgets.get(\"release_id\") if \"dbutils\" in dir() else None\n",
          "\n",
          "BLOB_PREFIX = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/US_Fred_Data/raw/{VINTAGE_DATE}/\"\n",
          "DELTA_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/US_Fred_Data/series_observations\"\n",
          "\n",
          "# Stats tracking\n",
          "stats = {\n",
          "    \"files_read\": 0,\n",
          "    \"total_rows\": 0,\n",
          "    \"duplicates_removed\": 0,\n",
          "    \"rows_written\": 0\n",
          "}\n",
          "\n",
          "print(\"=\"*80)\n",
          "print(\"SYNAPSE BLOB-TO-DELTA TRANSFORMER\")\n",
          "print(\"=\"*80)\n",
          "print(f\"Vintage: {VINTAGE_DATE}\")\n",
          "print(f\"Source: {BLOB_PREFIX}\")\n",
          "print(f\"Target: {DELTA_PATH}\")\n",
          "print()\n",
          "\n",
          "# Step 1: Read all parquet files from blob storage\n",
          "print(\"Step 1: Reading blob files...\")\n",
          "start_time = time.time()\n",
          "\n",
          "try:\n",
          "    # Read all parquet files with wildcard\n",
          "    df = spark.read.parquet(f\"{BLOB_PREFIX}*.parquet\")\n",
          "\n",
          "    stats[\"total_rows\"] = df.count()\n",
          "    print(f\"\u2705 Read {stats['total_rows']:,} rows from blob storage\")\n",
          "    print(f\"   Columns: {df.columns}\")\n",
          "\n",
          "except Exception as e:\n",
          "    print(f\"\u274c Error reading blobs: {e}\")\n",
          "    raise\n",
          "\n",
          "# Step 2: Add transformations\n",
          "print(\"\\nStep 2: Transforming data...\")\n",
          "\n",
          "# Note: Blob files have already been patched with:\n",
          "#   - release_date (FRED publication date)\n",
          "#   - collected_at (our collection timestamp, renamed from vintage_date)\n",
          "\n",
          "# Ensure required columns exist\n",
          "if \"release_date\" not in df.columns:\n",
          "    print(\"\u26a0\ufe0f  Warning: release_date missing, adding as null\")\n",
          "    df = df.withColumn(\"release_date\", lit(None))\n",
          "\n",
          "if \"collected_at\" not in df.columns:\n",
          "    print(\"\u26a0\ufe0f  Warning: collected_at missing, adding from VINTAGE_DATE parameter\")\n",
          "    df = df.withColumn(\"collected_at\", lit(VINTAGE_DATE))\n",
          "\n",
          "# Add partition columns for query performance\n",
          "transformed_df = df \\\n",
          "    .withColumn(\"year\", substring(col(\"date\"), 1, 4).cast(\"int\")) \\\n",
          "    .withColumn(\"month\", substring(col(\"date\"), 6, 2).cast(\"int\"))\n",
          "\n",
          "print(f\"\u2705 Added transformation columns\")\n",
          "print(f\"   Schema: {transformed_df.schema}\")\n",
          "\n",
          "# Step 3: Deduplicate (series_id + date)\n",
          "print(\"\\nStep 3: Deduplicating...\")\n",
          "before_dedup = transformed_df.count()\n",
          "deduped_df = transformed_df.dropDuplicates([\"series_id\", \"date\"])\n",
          "after_dedup = deduped_df.count()\n",
          "stats[\"duplicates_removed\"] = before_dedup - after_dedup\n",
          "\n",
          "print(f\"\u2705 Removed {stats['duplicates_removed']:,} duplicates\")\n",
          "print(f\"   Final rows: {after_dedup:,}\")\n",
          "\n",
          "# Step 4: Quality Control Checks\n",
          "print(\"\\nStep 4: Quality control checks...\")\n",
          "qc_passed = True\n",
          "\n",
          "# Check 1: Required columns\n",
          "# Note: Blob files after patching have: series_id, date, value, release_id, release_date, collected_at\n",
          "required_cols = [\"series_id\", \"date\", \"value\", \"release_id\", \"release_date\",\n",
          "                 \"collected_at\", \"year\", \"month\"]\n",
          "missing_cols = [c for c in required_cols if c not in deduped_df.columns]\n",
          "if missing_cols:\n",
          "    print(f\"\u274c Missing columns: {missing_cols}\")\n",
          "    qc_passed = False\n",
          "else:\n",
          "    print(f\"\u2705 Schema validation passed (all {len(required_cols)} required columns present)\")\n",
          "\n",
          "# Check 2: No nulls in key columns\n",
          "null_checks = deduped_df.select(\n",
          "    col(\"series_id\").isNull().cast(\"int\").alias(\"series_id_nulls\"),\n",
          "    col(\"date\").isNull().cast(\"int\").alias(\"date_nulls\"),\n",
          "    col(\"release_id\").isNull().cast(\"int\").alias(\"release_id_nulls\")\n",
          ").groupBy().sum().collect()[0]\n",
          "\n",
          "has_nulls = any(null_checks[i] > 0 for i in range(3))\n",
          "if has_nulls:\n",
          "    print(f\"\u274c Found nulls in key columns: {null_checks}\")\n",
          "    qc_passed = False\n",
          "else:\n",
          "    print(f\"\u2705 Null check passed\")\n",
          "\n",
          "# Check 3: Date format validation\n",
          "invalid_dates = deduped_df.filter(~col(\"date\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\")).count()\n",
          "if invalid_dates > 0:\n",
          "    print(f\"\u274c Found {invalid_dates} invalid date formats\")\n",
          "    qc_passed = False\n",
          "else:\n",
          "    print(f\"\u2705 Date format validation passed\")\n",
          "\n",
          "if not qc_passed:\n",
          "    raise Exception(\"Quality control failed!\")\n",
          "\n",
          "print(f\"\\n\u2705 All QC checks passed\")\n",
          "\n",
          "# Step 5: Write to Delta Lake\n",
          "print(f\"\\nStep 5: Writing to Delta Lake...\")\n",
          "print(f\"Target: {DELTA_PATH}\")\n",
          "print(f\"Partitioning by: (year, month)\")\n",
          "\n",
          "write_start = time.time()\n",
          "\n",
          "# Check if table exists and what partition it has\n",
          "try:\n",
          "    existing_table = DeltaTable.forPath(spark, DELTA_PATH)\n",
          "    print(f\"\u26a0\ufe0f  Table exists - will OVERWRITE\")\n",
          "\n",
          "    # Write with overwrite mode\n",
          "    deduped_df.write \\\n",
          "        .format(\"delta\") \\\n",
          "        .mode(\"overwrite\") \\\n",
          "        .partitionBy(\"year\", \"month\") \\\n",
          "        .option(\"overwriteSchema\", \"true\") \\\n",
          "        .save(DELTA_PATH)\n",
          "\n",
          "except Exception as e:\n",
          "    # Table doesn't exist, create new\n",
          "    print(f\"Table doesn't exist - creating new\")\n",
          "    deduped_df.write \\\n",
          "        .format(\"delta\") \\\n",
          "        .mode(\"overwrite\") \\\n",
          "        .partitionBy(\"year\", \"month\") \\\n",
          "        .save(DELTA_PATH)\n",
          "\n",
          "write_duration = time.time() - write_start\n",
          "stats[\"rows_written\"] = after_dedup\n",
          "\n",
          "print(f\"\u2705 Write complete in {write_duration:.2f}s\")\n",
          "print(f\"   Rows written: {stats['rows_written']:,}\")\n",
          "\n",
          "# Step 6: Optimize with Z-ordering\n",
          "print(f\"\\nStep 6: Optimizing Delta table...\")\n",
          "\n",
          "try:\n",
          "    delta_table = DeltaTable.forPath(spark, DELTA_PATH)\n",
          "\n",
          "    # Z-order by series_id and release_id\n",
          "    delta_table.optimize().executeZOrderBy(\"series_id\", \"release_id\")\n",
          "\n",
          "    print(f\"\u2705 Z-ordering complete (series_id, release_id)\")\n",
          "except Exception as e:\n",
          "    print(f\"\u26a0\ufe0f  Z-ordering skipped: {e}\")\n",
          "\n",
          "# Step 7: Benchmark queries\n",
          "print(f\"\\nStep 7: Benchmarking queries...\")\n",
          "\n",
          "# Query 1: Single series\n",
          "q1_start = time.time()\n",
          "single_series = spark.read.format(\"delta\").load(DELTA_PATH) \\\n",
          "    .filter(col(\"series_id\") == \"UNRATE\") \\\n",
          "    .count()\n",
          "q1_duration = (time.time() - q1_start) * 1000\n",
          "print(f\"  Single series query: {q1_duration:.0f}ms ({single_series} rows)\")\n",
          "\n",
          "# Query 2: Release query\n",
          "q2_start = time.time()\n",
          "release_data = spark.read.format(\"delta\").load(DELTA_PATH) \\\n",
          "    .filter(col(\"release_id\") == 10) \\\n",
          "    .select(\"series_id\", \"date\", \"value\") \\\n",
          "    .count()\n",
          "q2_duration = (time.time() - q2_start) * 1000\n",
          "print(f\"  Release query: {q2_duration:.0f}ms ({release_data} rows)\")\n",
          "\n",
          "# Query 3: Time range\n",
          "q3_start = time.time()\n",
          "time_range = spark.read.format(\"delta\").load(DELTA_PATH) \\\n",
          "    .filter((col(\"year\") == 2023) & (col(\"month\").isin([1, 2, 3]))) \\\n",
          "    .select(\"series_id\", \"date\", \"value\") \\\n",
          "    .count()\n",
          "q3_duration = (time.time() - q3_start) * 1000\n",
          "print(f\"  Time range query: {q3_duration:.0f}ms ({time_range} rows)\")\n",
          "\n",
          "# Query 4: Count\n",
          "q4_start = time.time()\n",
          "total_count = spark.read.format(\"delta\").load(DELTA_PATH).count()\n",
          "q4_duration = (time.time() - q4_start) * 1000\n",
          "print(f\"  Count query: {q4_duration:.0f}ms ({total_count:,} rows)\")\n",
          "\n",
          "# Step 8: Direct Series Analysis (preparing \"half the job\")\n",
          "print(f\"\\nStep 8: Running direct series analysis...\")\n",
          "analysis_start = time.time()\n",
          "\n",
          "delta_df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
          "\n",
          "# Analysis 1: Data Coverage Statistics\n",
          "print(\"\\n  Analysis 1: Data Coverage Statistics\")\n",
          "coverage_stats = delta_df.groupBy(\"series_id\").agg(\n",
          "    spark_min(\"date\").alias(\"first_date\"),\n",
          "    spark_max(\"date\").alias(\"last_date\"),\n",
          "    count(\"*\").alias(\"observation_count\"),\n",
          "    countDistinct(\"release_id\").alias(\"release_count\")\n",
          ").cache()\n",
          "\n",
          "print(f\"    Total series analyzed: {coverage_stats.count():,}\")\n",
          "\n",
          "# Sample coverage stats\n",
          "coverage_sample = coverage_stats.orderBy(col(\"observation_count\").desc()).limit(10)\n",
          "print(f\"    Top 10 series by observation count:\")\n",
          "for row in coverage_sample.collect():\n",
          "    print(f\"      {row.series_id}: {row.observation_count:,} obs, {row.first_date} to {row.last_date}\")\n",
          "\n",
          "# Analysis 2: Temporal Distribution\n",
          "print(\"\\n  Analysis 2: Temporal Distribution\")\n",
          "temporal_dist = delta_df.groupBy(\"year\", \"month\").agg(\n",
          "    count(\"*\").alias(\"observation_count\"),\n",
          "    countDistinct(\"series_id\").alias(\"unique_series\")\n",
          ").orderBy(\"year\", \"month\")\n",
          "\n",
          "recent_months = temporal_dist.filter(col(\"year\") >= 2020).collect()\n",
          "if recent_months:\n",
          "    print(f\"    Recent months distribution (2020+):\")\n",
          "    for row in recent_months[-12:]:  # Last 12 months\n",
          "        print(f\"      {row.year}-{row.month:02d}: {row.observation_count:,} obs, {row.unique_series:,} series\")\n",
          "\n",
          "# Analysis 3: Release Statistics\n",
          "print(\"\\n  Analysis 3: Release Statistics\")\n",
          "release_stats = delta_df.groupBy(\"release_id\").agg(\n",
          "    countDistinct(\"series_id\").alias(\"series_count\"),\n",
          "    count(\"*\").alias(\"observation_count\"),\n",
          "    spark_min(\"date\").alias(\"earliest_data\"),\n",
          "    spark_max(\"date\").alias(\"latest_data\")\n",
          ").orderBy(col(\"series_count\").desc())\n",
          "\n",
          "print(f\"    Total releases: {release_stats.count()}\")\n",
          "print(f\"    Top 5 releases by series count:\")\n",
          "for row in release_stats.limit(5).collect():\n",
          "    print(f\"      Release {row.release_id}: {row.series_count:,} series, {row.observation_count:,} obs\")\n",
          "\n",
          "# Analysis 4: Value Statistics (basic descriptive stats)\n",
          "print(\"\\n  Analysis 4: Value Statistics\")\n",
          "value_stats = delta_df.select(\"series_id\", \"value\").groupBy(\"series_id\").agg(\n",
          "    count(\"*\").alias(\"n_obs\"),\n",
          "    avg(\"value\").alias(\"mean_value\"),\n",
          "    stddev(\"value\").alias(\"stddev_value\"),\n",
          "    spark_min(\"value\").alias(\"min_value\"),\n",
          "    spark_max(\"value\").alias(\"max_value\")\n",
          ").cache()\n",
          "\n",
          "print(f\"    Computed statistics for {value_stats.count():,} series\")\n",
          "\n",
          "# Sample some volatile series (high stddev/mean ratio)\n",
          "volatile_series = value_stats.filter(\n",
          "    (col(\"stddev_value\").isNotNull()) &\n",
          "    (col(\"mean_value\") != 0) &\n",
          "    (col(\"n_obs\") > 100)\n",
          ").withColumn(\n",
          "    \"cv\", col(\"stddev_value\") / col(\"mean_value\")\n",
          ").orderBy(col(\"cv\").desc()).limit(10)\n",
          "\n",
          "print(f\"    Top 10 most volatile series (coefficient of variation):\")\n",
          "for row in volatile_series.collect():\n",
          "    print(f\"      {row.series_id}: CV={row.cv:.2f}, mean={row.mean_value:.2f}, stddev={row.stddev_value:.2f}\")\n",
          "\n",
          "# Save analysis results to separate Delta tables for later use\n",
          "print(\"\\n  Saving analysis results...\")\n",
          "ANALYSIS_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/US_Fred_Data/analysis\"\n",
          "\n",
          "# Save coverage stats\n",
          "coverage_stats.write.format(\"delta\").mode(\"overwrite\") \\\n",
          "    .save(f\"{ANALYSIS_PATH}/series_coverage\")\n",
          "print(f\"    \u2705 Coverage stats saved to {ANALYSIS_PATH}/series_coverage\")\n",
          "\n",
          "# Save value statistics\n",
          "value_stats.write.format(\"delta\").mode(\"overwrite\") \\\n",
          "    .save(f\"{ANALYSIS_PATH}/value_statistics\")\n",
          "print(f\"    \u2705 Value statistics saved to {ANALYSIS_PATH}/value_statistics\")\n",
          "\n",
          "# Save release statistics\n",
          "release_stats.write.format(\"delta\").mode(\"overwrite\") \\\n",
          "    .save(f\"{ANALYSIS_PATH}/release_statistics\")\n",
          "print(f\"    \u2705 Release statistics saved to {ANALYSIS_PATH}/release_statistics\")\n",
          "\n",
          "analysis_duration = time.time() - analysis_start\n",
          "print(f\"\\n  \u2705 Analysis complete in {analysis_duration:.2f}s\")\n",
          "print(f\"  Analysis results available at: {ANALYSIS_PATH}\")\n",
          "\n",
          "# Final summary\n",
          "total_duration = time.time() - start_time\n",
          "\n",
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"SYNAPSE BLOB-TO-DELTA TRANSFORMATION + ANALYSIS COMPLETE\")\n",
          "print(\"=\"*80)\n",
          "print(f\"Vintage: {VINTAGE_DATE}\")\n",
          "print(f\"Total rows read: {stats['total_rows']:,}\")\n",
          "print(f\"Duplicates removed: {stats['duplicates_removed']:,}\")\n",
          "print(f\"Total rows written: {stats['rows_written']:,}\")\n",
          "print(f\"Total duration: {total_duration:.2f}s\")\n",
          "print()\n",
          "print(\"Query Performance:\")\n",
          "print(f\"  Single series: {q1_duration:.0f}ms\")\n",
          "print(f\"  Release: {q2_duration:.0f}ms\")\n",
          "print(f\"  Time range: {q3_duration:.0f}ms\")\n",
          "print(f\"  Count: {q4_duration:.0f}ms\")\n",
          "print()\n",
          "print(\"Analysis Results:\")\n",
          "print(f\"  Coverage stats: {ANALYSIS_PATH}/series_coverage\")\n",
          "print(f\"  Value statistics: {ANALYSIS_PATH}/value_statistics\")\n",
          "print(f\"  Release statistics: {ANALYSIS_PATH}/release_statistics\")\n",
          "print(\"=\"*80)\n",
          "print(f\"\\n\u2705 Delta Lake ready at: {DELTA_PATH}\")\n",
          "print(f\"\u2705 Analysis tables ready at: {ANALYSIS_PATH}\")\n",
          "print(f\"\\n\ud83d\udcca Half the analysis job is now done - ready for downstream insights!\")\n",
          "\n"
        ],
        "outputs": [],
        "execution_count": null
      }
    ]
  }
}